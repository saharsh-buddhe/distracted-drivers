
### Get our data and flatten it
(train_data, train_labels) = get_train_data(flatten = True)
(test_data, test_labels) = get_test_data(flatten = True)


knn  = KNeighborsClassifier(n_neighbors=5)
dt   = DecisionTreeClassifier(max_depth = 10)

knn.fit(train_data, train_labels)
dt.fit(train_data, train_labels)

# they might use accuracy_score
predictions = knn.predict(test_data)
score = accuracy_score(test_labels, predictions)
print(score)

# they might wrap accuracy_score in their own definition
def score_model(model, test_data, test_labels):
  predictions = model.predict(test_data)
  score = accuracy_score(test_labels, predictions)
  print(score)
  
# or they might use the model's scoring function
print(knn.score(test_data, test_labels))

# students should see if they can get up to 75% accuracy!
# and should tabulate across parameters

print('KNN score: %0.2f'%knn.score(test_data, test_labels))
print('DT score: %0.2f'%dt.score(test_data, test_labels))

# I suggest ...
# * knn with neighbors of 5, 10, 100, 1000
# * MLP with 1, 3, 5 layers with 4, 10, 30
# * Decision tree with max_depth of 2, 5, 10
